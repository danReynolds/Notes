# Paging: Faster Translations (TLBs)
Using paging as the core mechanism to support virtual memory can lead to high performance overheads.

Chopping the address space into small, fixed-size units (pages), requires a large amount of mapping information.

Because that information is generally stored in physical memory, paging requires an extra memory lookup for each virtual address generated by the program.

Go from VPN to PTE in physical memory, to PFN, there is now a middleman that adds an extra memory lookup every time.

Going to memory for translation information every instruction fetch or explicit load is prohibitively slow.

## How to Speed Up Address Translation
How can address translation be sped up to avoid the extra memory reference that paging seems to require?

To speed up address translation, we must add a **translation-lookaside buffer** or (**TLB**). A TLB is a part of the chip's **memory-management unit (MMU)** and is a hardware **cache** of popular virtual to physical address translations, and a better name would be an **address-translation cache.**

With each virtual memory reference, the hardware first checks the TLB to see if the desired translation is within it, if so the translation is performed quickly **without** having to go to the **page table** which has all translations.

The improvement that TLBs make to performance is what makes virtual memory possible.

## TLB Basic Algorithm
Assuming a simple **linear page table**, and a **hardware-managed TLB**, we could now check the TLB:

		VPN = (VirtualAddress & VPN_MASK) >> SHIFT
		(Success, TlbEntry) = TLB_lookup(VPN)
		if (Success == True) // TLB Hit
			if (CanAccess(TlbEntry.ProtectBits) == True)
				Offset = VirtualAddress & OFFSET_MASK
				PhysAddr = (TlbEntry.PFN << SHIFT) | Offset
				AccessMemory(PhysAddr)
			else
				RaiseException(PROTECTION_FAULT)
		else
			PTEAddr = PTBR (page table base register) + (VPN * sizeof(PTE))
			PTE = AccessMemory(PTEAddr) // additional memory access
			if (PTE.Valid == False)
				RaiseException(SEGMENTATION_FAULT)
			else if (CanAccess(PTE.ProtectBits) == False)
				RaiseException(PROTECTION_FAULT)
			else
				TLB_Insert(VPN, PTE.PFN, PTE.ProtectBits)
				RetryInstruction()

The first thing it does is extract the VPN from the virtual address and check if the TLB holds the translation for this VPN. If it does, it is a **TLB Hit**, and the page frame number (PFN) can be extracted from the TLB, adding the offset and accessing memory.

If the CPU does not find the translation in the TLB, **TLB miss**, the hardware accesses the page table to find the translation, and assuming that the virtual memory reference generated by the process is valid and accessible, it updates the TLB to now have the translation its its cache.

A miss is more costly because of the extra memory reference needed to access the page table.

Once the TLB is updated, the hardware retries the instruction, this time the translation is immediately found in the TLB and the memory reference is processed quickly.

The TLB cache is built on the premise that in the common case, translations are found in the cache, so little overhead is added, as the TLB is found near the processing core and is designed to be very fast.

When a miss occurs, the high cost of paging is incurred, the page table must be accessed to find the translation, and an extra memory reference occurs.

If this happens often, the program will likely run much slower, as memory accesses are costly. It is therefore important to avoid TLB misses whenever possible.

## Accessing an Array
Assume there is an array of 10 4-byte integers in memory, starting at virtual address 100, or binary (0110 0100). There is also a small 8-bit virtual address space, with 16 virtual pages, each holding 16-bytes. Then the first 4-bits of the virtual address is the VPN and the last 4 bits are the offset.

If the first element in the array begins at VPN=06, offset 04, since binary `0110` is 06 and `0100` for the offset is 4, only three 4-byte integers fit onto that page.

The array continues onto the next page, VPN=07, where the next four entries are found.

Finally, the last three entries are located on the next page of the address space, VPN=08.

Now considering a simple look that accesses each array element:

		int sum = 0;
		for (i = 0; i < 10; i ++) {
			sum += a[i];
		}

The hardware extracts the VPN from this, `100 decimal=0110 0100 binary`, and we get a VPN=06, using that to check the TLB for a valid translation.

Assuming this is is the first time the program accesses the array, the result will be a TLB miss.

The next access to element 1 of the array results is a TLB hit since the second element of the array can fit on the same page.

The next elemnent also succeeds, but `a[3]` fails, since address 116 `0111 0000` is on the next page. `a[4..6]` will then hit, however, and the only other miss occurs for `a[7]` since it is also on a different page. `a[8..9]` hit since they are on the same page as `a[7]`.

The TLB hits and misses end up being:

m,h,h,m,h,h,h,m,h,h

The **hit rate** for the TLB would then be 70%. While this is still low, it is better than 0 because of **spatial locality**. The elements of this array are located spatially close to one-another and therefore the only first access to an element on a page results in a TLB miss.

Note the importance of the page size, if the page size had been larger, even fewer misses would have occurred.

As typical page sizes are more like 4KB, these types of dense array-based accesses achieve excellent TLB performance.

Another point to make is that if the program, soon after the loop completes, accesses the array again, there would be even better results, assuming the TLB was large enough.

if it cached all three of those pages, then the hit rate would be 100%.

In this case, the TLB hit rate would be high because of **temporal locality**, the quick referencing of memory items in time.

**Spatial locality = where memory accessed**, **Temporal locality = when memory accessed **.

Like any cache, TLBs rely on both spatial and temporal locality for success, which are program properties.

If the program of interest exhibits these localities, then the TLB hit rate will be high.

## Use Caching Whenever Possible
Caching is one of the most fundamental performance techniques. Hardware caches take advantage of **locality**. 

Hardware caches take advantage of locality by keeping copies of memory in small, fast on-chip memory.

Note that to have a fast cache, it **must be small**. Any large cache is by definition slow and defeats the purpose.

## Who Handles TLB Misses?
Either the hardware or OS. The hardware used to have complex instruction sets called **CISC**, complex instruction set computers, and the people who built the hardware didn't trust the OS people much, therefore the hardware would handle the miss entirely.

To do this, the hardware has to know *exactly* where the page tables are located in memory,via a **page table base register.**

On a miss, the hardware would walk the page table, find the correct page table entry (PTE) and extract the desired translation, update the TLB with the translation and retry.

Modern architectures use **RISC**, reduced instruction set computers, that have **software-managed TLBs**. On a TLB miss, the hardware simply raises an exception which pauses the current instruction stream and jumps to a trap handler.

The trap handler looks up the translation in the page table, uses privileged instructions to update the TLB and returns from the trap. The hardware then retries the instruction.

In this case, as opposed to normal system calls, the PC must not be incremented and the program should resume exactly where it was, so that it can try to retrieve from the TLB again instead of moving on to the next instruction.

The OS also needs to be careful to not let an infinite chain of TLB misses occur if it tries to translate the location of the trap handlers as well.

This can be dealt with by keeping the TLB trap handlers in physical memory, **unmapped** and not subject to address translation, or reserve some entries in the TLB for permanently-valid translations and use some of those for the handler code.

The main advantage of using the OS is the flexibility. The OS can use any data structure it wants to implement the page table, and its simple. The hardware doesn't have to do much on a miss, just raise an exception and the OS TLB miss handler does the rest.

## TLB Contents: What's In There?
A typical TLB might have 32,64,128 entries and be **fully-associative**, which means that any given translation can be anywhere in the TLB and that the hardware will search the entire TLB in parallel to find the desired translation.

A TLB entry can look like:

VPN | PFN | other bits

so that it can check the VPN match, return the PFN translation, and also have additional data.

## TLB Valid Bit
A TLB valid bit represents whether a TLB has a valid entry in it (a mapping from a VPN to a PFN.

When a system boots, each entry will be marked invalid, since no address translations are in the cache yet.

Once programs run and virtual address spaces are accessed, the TLB gets populated and more entries become valid.

The TLB valid bit is useful when performing a context switch. By setting all TLB entries to invalid, the system can ensure that the about-to-be-run process does not accidentally use a virtual-to-physical translation from a previous process.

A TLB usually also has **protection** bits, which determine how a page can be accessed. Code pages might be marked read and execute, whereas heap pages might be read and write.

There could also be dirty bits, etc.

## TLB Issue: Context Switch
When switching between processes, and consequently, address spaces, the TLB contains translations that are only meaningful for certain processes.

When switching from one process to another, the hardware, or OS or both must be careful to ensure that the about-to-be-run process does not accidentally use translations from the previously run process.

For example, if P1 is running, it assumes the TLB might be caching translations that are valid for it, that come from P1's page table. Assume that the 10th virtual page of P! is mapped to physical frame 100.

Now if P2 also exists, and the OS context switches to P2, here the 10th virtual page of P2 is mapped to physical frame 170 in its page table. If entries for both processes were in the TLB, the contents of the TLB would be:

VPN 10, PFN 100, valid 1
VPN 10, PFN 170, valid 1

VPN 10 translates to **either** PFN 100 or PFN 170, but the hardware can't distinguish which entry is mean for which process.


## Managing TLB Contents on a Context Switch
One approach is to **flush** the TLB on context switches, emptying it before the next process starts.

On a software system, this can be accomplished with a privileged hardware instruction.

With a hardware-managed TLB, the flush could be enacted when the page-table base register is changed.

In either case, the flush operation sets all bits to 0 and clears the contents of the TLB.

By flushing the TLB on each context switch, it gives a working solution, as a process will never accidentally encounter the wrong translations in the TLB.

However, each time a process runs, it must incur TLB misses as it touches its data and code pages. If the OS switches between processes frequently, this cost could be high.

To reduce this overhead, some systems add hardware support to enable sharing of the TLB across context switches.

An **Address Space Identifier (ASID)** is like a **Process Identifier (PID)** but usually has fewer bits than a PID.

If we take the TLB entries and add ASIDs:

VPN 10, PFN 100, valid 1, ASID 1
VPN 10, PFN 170, valid 1, ASID 2

then processes can readily share the TLB.

The hardware just needs to know which process is running in order to perform translations, and the OS must set some privileged register on a context switch to the ASID of the current process.

## Replacement Policy
Another issue with the TLB is **cache replacement**. When we are installing a new entry in the TLB, we have to **replace** an old one, which one do we replace?

Which TLB entry should be replaced when we add a new TLB entry?

The goal is to minimize the **miss rate** or increase the **hit rate**.

One common approach is to evict the **least-recently-used or LRU** entry.

Someone who hasn't been used recently is the best candidate for eviction.

Another approach is to do it randomly, it avoids edge-cases, and is simple.

The LRU case is worse than random in a situation where a program is looping over n+1 pages with a TLB of size n. It will keep evicting the VPN it will reference next.

## RAM Isn't Always RAM
The term **random access memory** implies that you can access any part of RAM as quickly as any other. While it is generally good to think of RAM in this way, because of hardware/OS features such as the TLB, accessing a particular page of memory may be costly, particularly if the page isn't currently in the TLB.

Sometimes randomly accessing an address space, particularly if the number of pages exceed TLB coverage, can lead to severe performance penalties. 

**Culler's Law: RAM** isn't always **RAM**.

## Conclusion
Hardware can make address translation faster thanks to a small, dedicated on-chip TLB that acts as an address translation cache.

Most memory references will hopefully be made without have to access the page table.

You get into trouble if the number of page accesses by a program exceeds the size of the TLB.

This is called exceeding **TLB Coverage**. One solution is to include support for larger page sizes, by mapping key data structures into regions of the program's address space that are mapped by larger pages, the effective coverage of the TLB can be increased.

Support for larger pages is often exploited by programs like **database management systems (DBMS)**, which have certain data structures that are both large and randomly accessed.

TLB access can easily become a bottleneck in the CPU pipeline. With a **physically-indexed cache**, address translation has to take place before the cache is accessed, which can slow things down. People have looked into clever ways to access caches with virtual addresses, avoiding the expensive step of translation in the case of a cache hit.

Such **virtually-indexed caches** solve some performance problems, but introduce others. And so we move on.





